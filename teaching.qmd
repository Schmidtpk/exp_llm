---
title: "LLM Seminar: Workflow & Tech Stack"
format: 
  revealjs:
    html-math-method: katex
    theme: simple
    slide-number: true
    preview-links: auto
    overview: true
    width: 1280
    height: 720
    fontsize: 2.5em
    scrollable: true  # Erlaubt Scrollen, wenn der Code zu groß ist
    highlight-style: github # oder 'monokai', 'dracula', 'solarized'
execute:
  echo: true
  eval: false
---
<style>
/* Vergrößert den Code-Block */
.reveal pre code {
  font-size: 1.5em !important; /* Faktor der Vergrößerung */
  line-height: 1.3 !important; /* Zeilenabstand für Lesbarkeit */
}
</style>

# Mindset & Prozess

## Wissenschaftliches Arbeiten

- **Es ist ein Prozess**
- Der nächste Code/Satz muss besser sein als der letzte
- Nicht "perfekt" – nur besser
- Fast jeder Code/Satz ist besser als *keiner*

## KI: "Vibe Coding" & Writing

- **Nutzt KI!**
- Aber: Gebt die Verantwortung nicht ab
- Lieber kleine Schritte mit Feedback:
  - Planung $\rightarrow$ Struktur $\rightarrow$ Absatz $\rightarrow$ Satz
- Langsames Arbeiten statt Sackgasse
  - Wir wollen oft zu schnell Resultate
  - "Aufräumen" nach KI ist anstrengender als KI lenken

## Meta-Cognition

- Ständige Selbstverortung ist wichtig:
- *Wo bin ich gerade?*
- *Wo will ich hin?*
- *Was ist der exakt nächste Schritt?*

# Experimentelle Forschung

## Grundkonzepte

- **Treatment vs. Control**
- **Average Treatment Effect (ATE)**
- Besonderheit bei LLMs:
  - Fast deterministisch
  - Kleine Sample Size (oft $n=1$) ist okay
  - nicht nötig, aber wäre schön: Replizierbarkeit durch "Seed"

## Design-Typen

- **Faktorielles Design (Orthogonal)**
  - Mehrere Dimensionen gleichzeitig testen
  - z.B. *Model (GPT vs. Mistral)* $\times$ *Prompt (Kurz vs. Lang)*
  
- **Vergleichendes Design ("Parallel")**
  - Eine Dimension, viele Ausprägungen
  - z.B. *Treatment 1, Treatment 2, Treatment 3* (verschiedene Prompt-Strategien)

# Workflow

## Der ideale Ablauf

1. **Forschungsfrage** definieren
2. **Einfaches LLM Programm** schreiben (Setup)
3. **Einfachstes laufendes Experiment** (MVP)
4. Erst dann: **Erweiterungen**

## Entscheidungskriterien für Erweiterungen

Bevor ihr das Experiment komplexer macht:

- Ist es **interessant**? (Erkenntnisgewinn)
- Ist es **machbar**? (Zeit/Budget/Skill)
- Ist es **konsistent**? (Passt es logisch in Code & Arbeit)

# Software Stack

## Übersicht

- **Sprache:** R oder Python
- **Location:** Cloud oder Local
  - Local: RStudio, VSCode, Posit
  - Cloud: Colab, RStudio Server
- **KI Tools:**
  - Chats (Copy/Paste)
  - Agentic: Cursor, Claude Code, Gemini CLI
  - Autocomplete: Github Copilot

## Mainstream Packages

**R (Tidyverse Ecosystem)**

- `tidyverse` (Datenmanipulation & Plots)
- `ellmer` (Einfache LLM Interaktion)

**Python (Data Science Stack)**

- `openai` (API Client)
- `pandas` (Datenhaltung)
- `matplotlib` (Visualisierung)

# Code Beispiele: Setup

## Setup in R

Nutzt `ellmer` für schnellen Start.

```{r}
# setup.R
library(ellmer)

# Sys.setenv(OPENROUTER_API_KEY = "YOUR_KEY")

# Create chat instance
chat <- chat_openrouter(
  model = "mistralai/mixtral-8x7b-instruct",
  system_prompt = "You are a helpful and creative assistant."
)

# Start conversation
response <- chat$chat("Who is talking to me?")
print(response)
```

## Setup in Python

Der Standard-Weg via `openai` Library.

```{python}
import os
from openai import OpenAI

# 1. ---- CONFIGURATION ----
# Setzen Sie den API Key direkt in die Umgebungsvariablen
# os.environ["OPENROUTER_API_KEY"] = "sk-ihr-key-hier"


# Initialisieren des Clients mit OpenRouter Base URL
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.environ["OPENROUTER_API_KEY"],
)

SYSTEM_PROMPT = "You are a helpful assistant."
USER_MESSAGE = "Who am I talking to?"
MODEL = "mistralai/mixtral-8x7b-instruct"

# 2. ---- API REQUEST ----
response = client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": USER_MESSAGE}
    ]
)

# 3. ---- PRINT RESPONSE ----
reply = response.choices[0].message.content
print(reply)
```

# Code Beispiele: Expectations

## Experiment in R 


```{r}
# expectations.R - Setup
library(ellmer)

model <- "openai/gpt-oss-120b"
n <- 3

events <- c(
  "Trump will be president in 2028",
  "AI will pass the Turing test by 2027",
  "Bitcoin price will exceed $100,000 by end of 2026"
)

# Strukturierte Antwort erzwingen
response_type <- type_object(
  min_prob = type_number("Minimum probability"),
  prob = type_number("Point probability estimate"),
  max_prob = type_number("Maximum probability")
)

results_list <- list()
counter <- 1

for (i in seq_along(events)) {
  for (j in 1:n) {
    chat <- chat_openrouter(
      model = model,
      system_prompt = "You are an expert forecaster. JSON only."
    )
    
    prompt <- sprintf("What is the probability that: %s?", events[i])
    result <- chat$chat_structured(prompt, type = response_type)

    results_list[[counter]] <- data.frame(
      event = events[i], run = j, 
      p = result$prob, minp = result$min_prob, maxp = result$max_prob
    )
    counter <- counter + 1
  }
}
final_predictions <- do.call(rbind, results_list)
```

## Experiment in Python 


```{python}
# expectations.py
import os
import json
import pandas as pd
import matplotlib.pyplot as plt
from openai import OpenAI

# Setup API --------------------------
# os.environ["OPENROUTER_API_KEY"] = "sk-ihr-key-hier"

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.environ["OPENROUTER_API_KEY"],
)

SYSTEM_PROMPT = "You are an expert forecaster providing probability estimates. Reply with only the requested JSON structure and nothing else."
MODEL = "openai/gpt-oss-120b"

# Define events --------------------------

events = [
    "Trump will be president in 2028",
    "Global average temperature will increase by more than 1.5°C by 2030",
    "AI will pass the Turing test by 2027",
    "Bitcoin price will exceed $100,000 by end of 2026",
    "Humans will land on Mars by 2035"
]

n = 3  # Anzahl der Wiederholungen

# 3. Experiment durchführen --------------------------
results_list = []
print(f"Frage {len(events) * n} Prompts ab...")

for event in events:
    for i in range(n):
        print(f"  ... {event[:15]}... (Run {i+1}/{n})")
        
        prompt = (f"Probability for: '{event}'? Give 0-100%. "
                  'Return valid JSON object with keys: "min_prob", "prob", "max_prob".')

        try:
            response = client.chat.completions.create(
                model= MODEL,
                messages=[
                          {"role": "system", "content": SYSTEM_PROMPT},
                          {"role": "user", "content": prompt}
                        ],
                response_format={"type": "json_object"}
            )

            # Robustes Parsing
            content = response.choices[0].message.content
            start = content.find('{')
            end = content.rfind('}') + 1
            
            if start != -1:
                data = json.loads(content[start:end])
                
                results_list.append({
                    "event": event,
                    "run": i,
                    "min": data["min_prob"],
                    "p": data["prob"],
                    "max": data["max_prob"]
                })
        except Exception as e:
            print(f"    Fehler im Run {i+1}: {e}")

# 4. Plotten --------------------------
if results_list:
    df = pd.DataFrame(results_list)
    
    plt.figure(figsize=(10, 6))
    
    # Wir weisen jedem Event eine Nummer auf der Y-Achse zu (0, 1, 2...)
    events_unique = df["event"].unique()
    y_map = {evt: i for i, evt in enumerate(events_unique)}
    
    # Plotten Loop
    for idx, row in df.iterrows():
        base_y = y_map[row["event"]]
        # Kleiner Versatz je nach Run, damit Punkte nicht übereinander liegen
        # Run 0 -> -0.1, Run 1 -> 0.0, Run 2 -> +0.1
        offset = (row["run"] - 1) * 0.1 
        
        plt.errorbar(
            x=row["p"],
            y=base_y + offset,
            xerr=[[row["p"] - row["min"]], [row["max"] - row["p"]]],
            fmt='o',
            color='tab:blue',
            capsize=4
        )

    # Achsenbeschriftung korrigieren
    plt.yticks(range(len(events_unique)), events_unique)
    plt.xlabel("Wahrscheinlichkeit (%)")
    plt.title(f"LLM Vorhersagen ({n} Runs pro Event)")
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.show()
else:
    print("Keine Ergebnisse.")
```
